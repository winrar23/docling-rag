# docling-rag MVP Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** –ü–æ—Å—Ç—Ä–æ–∏—Ç—å CLI-—É—Ç–∏–ª–∏—Ç—É `docling-rag` –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (PDF, DOCX, MD, TXT) —á–µ—Ä–µ–∑ Docling + Sentence Transformers + NumPy.

**Architecture:** –ú–æ–¥—É–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: `parser ‚Üí chunker ‚Üí embedder ‚Üí file_storage`. CLI-–∫–æ–º–∞–Ω–¥—ã (`init`, `add`, `search`, `list`) –≤—ã–∑—ã–≤–∞—é—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é `storage.py`. –•—Ä–∞–Ω–∏–ª–∏—â–µ ‚Äî `.npy` –º–∞—Ç—Ä–∏—Ü–∞ + `metadata.json`.

**Tech Stack:** Python 3.10+, Docling, sentence-transformers (all-MiniLM-L6-v2), NumPy, Click, pytest

---

## Task 0: Project Scaffold

**Files:**
- Create: `pyproject.toml`
- Create: `core/__init__.py`
- Create: `storage/__init__.py`
- Create: `cli/__init__.py`
- Create: `tests/__init__.py`
- Create: `tests/core/__init__.py`
- Create: `tests/storage/__init__.py`
- Create: `config.yaml`

**Step 1: –°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π**

```bash
cd "/Users/danny/Documents/–î–æ–∫—É–º–µ–Ω—Ç—ã –î–∞–Ω–∏–∏–ª/Github/Docling RAG"
mkdir -p core storage cli tests/core tests/storage data/documents logs
touch core/__init__.py storage/__init__.py cli/__init__.py
touch tests/__init__.py tests/core/__init__.py tests/storage/__init__.py
```

**Step 2: –°–æ–∑–¥–∞—Ç—å `pyproject.toml`**

```toml
[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.backends.legacy:build"

[project]
name = "docling-rag"
version = "0.1.0"
description = "Semantic search CLI for technical documentation using Docling"
requires-python = ">=3.10"
dependencies = [
    "docling>=2.0.0",
    "sentence-transformers>=3.0.0",
    "numpy>=1.26.0",
    "click>=8.1.0",
    "pyyaml>=6.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-mock>=3.12.0",
]

[project.scripts]
docling-rag = "cli:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["core*", "storage*", "cli*"]
```

**Step 3: –°–æ–∑–¥–∞—Ç—å `config.yaml` —Å –¥–µ—Ñ–æ–ª—Ç–∞–º–∏**

```yaml
embedding_model: all-MiniLM-L6-v2
chunk_size: 800        # —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä chunk –≤ —Ç–æ–∫–µ–Ω–∞—Ö (‚âà 3200 —Å–∏–º–≤–æ–ª–æ–≤)
chunk_overlap: 80      # overlap (10% –æ—Ç chunk_size)
top_k_results: 5
data_dir: data
log_file: logs/search.log
```

**Step 4: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**

```bash
cd "/Users/danny/Documents/–î–æ–∫—É–º–µ–Ω—Ç—ã –î–∞–Ω–∏–∏–ª/Github/Docling RAG"
uv pip install -e ".[dev]"
```

–û–∂–∏–¥–∞–µ–º: —É—Å–ø–µ—à–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–µ–∑ –æ—à–∏–±–æ–∫.

**Step 5: Commit**

```bash
git add pyproject.toml config.yaml core/ storage/ cli/ tests/ .gitignore CLAUDE.md docs/
git commit -m "chore: initial project scaffold with pyproject.toml and directory structure"
```

---

## Task 1: core/chunker.py

**Files:**
- Create: `tests/core/test_chunker.py`
- Create: `core/chunker.py`

### –ß—Ç–æ –¥–µ–ª–∞–µ—Ç chunker

–ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ `DoclingElement` (—É–ø—Ä–æ—â—ë–Ω–Ω–æ: dict —Å –ø–æ–ª—è–º–∏ `text`, `type`, `page`).
–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ `Chunk` ‚Äî dataclass —Å –ø–æ–ª—è–º–∏ `text`, `source_file`, `chunk_id`, `page_number`, `element_type`.

–ü—Ä–∞–≤–∏–ª–∞:
- `type == "table"` –∏–ª–∏ `type == "code"` ‚Üí –∞—Ç–æ–º–∞—Ä–Ω—ã–π chunk (–Ω–µ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è)
- `type == "text"` ‚Üí –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ ~`chunk_size * 4` —Å–∏–º–≤–æ–ª–æ–≤, –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π chunk —Å `overlap` —Å–∏–º–≤–æ–ª–∞–º–∏ –∏–∑ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ

**Step 1: –ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ—Å—Ç—ã**

```python
# tests/core/test_chunker.py
import pytest
from core.chunker import Chunk, chunk_elements


def make_element(text, etype="text", page=1):
    return {"text": text, "type": etype, "page": page}


def test_chunk_returns_list_of_chunks():
    elements = [make_element("Hello world. This is a test.")]
    result = chunk_elements(elements, source_file="doc.pdf", chunk_size=3200, overlap=80)
    assert isinstance(result, list)
    assert len(result) > 0
    assert isinstance(result[0], Chunk)


def test_chunk_has_required_fields():
    elements = [make_element("Hello world.")]
    result = chunk_elements(elements, source_file="doc.pdf", chunk_size=3200, overlap=80)
    chunk = result[0]
    assert chunk.source_file == "doc.pdf"
    assert chunk.chunk_id == 0
    assert chunk.page_number == 1
    assert chunk.element_type == "text"
    assert "Hello world" in chunk.text


def test_table_is_atomic_chunk():
    elements = [make_element("col1 | col2\n----|----\nA | B", etype="table")]
    result = chunk_elements(elements, source_file="doc.pdf", chunk_size=100, overlap=10)
    assert len(result) == 1
    assert result[0].element_type == "table"


def test_code_is_atomic_chunk():
    elements = [make_element("SELECT * FROM users WHERE id = 1", etype="code")]
    result = chunk_elements(elements, source_file="doc.pdf", chunk_size=100, overlap=10)
    assert len(result) == 1
    assert result[0].element_type == "code"


def test_long_text_is_split_into_multiple_chunks():
    long_text = "Sentence number {}. " * 200
    elements = [make_element(long_text.format(*range(200)))]
    result = chunk_elements(elements, source_file="doc.pdf", chunk_size=200, overlap=20)
    assert len(result) > 1


def test_overlap_carries_context():
    sentence = "The quick brown fox. "
    elements = [make_element(sentence * 100)]
    result = chunk_elements(elements, source_file="doc.pdf", chunk_size=200, overlap=40)
    if len(result) > 1:
        # –ö–æ–Ω–µ—Ü –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ –¥–æ–ª–∂–µ–Ω –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –Ω–∞—á–∞–ª–µ –≤—Ç–æ—Ä–æ–≥–æ
        end_of_first = result[0].text[-40:]
        assert end_of_first in result[1].text or len(result[1].text) > 0


def test_empty_elements_returns_empty_list():
    result = chunk_elements([], source_file="doc.pdf", chunk_size=3200, overlap=80)
    assert result == []


def test_chunk_ids_are_sequential():
    elements = [make_element("Text " * 500)]
    result = chunk_elements(elements, source_file="doc.pdf", chunk_size=200, overlap=20)
    ids = [c.chunk_id for c in result]
    assert ids == list(range(len(result)))
```

**Step 2: –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã ‚Äî —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –ø–∞–¥–∞—é—Ç**

```bash
pytest tests/core/test_chunker.py -v
```

–û–∂–∏–¥–∞–µ–º: `ImportError: cannot import name 'Chunk' from 'core.chunker'`

**Step 3: –ù–∞–ø–∏—Å–∞—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é `core/chunker.py`**

```python
from dataclasses import dataclass
from typing import Any


@dataclass
class Chunk:
    text: str
    source_file: str
    chunk_id: int
    page_number: int
    element_type: str  # "text", "table", "code"


def chunk_elements(
    elements: list[dict[str, Any]],
    source_file: str,
    chunk_size: int = 3200,   # —Å–∏–º–≤–æ–ª—ã (‚âà800 —Ç–æ–∫–µ–Ω–æ–≤ √ó 4 —Å–∏–º–≤–æ–ª–∞/—Ç–æ–∫–µ–Ω)
    overlap: int = 320,        # —Å–∏–º–≤–æ–ª—ã (‚âà80 —Ç–æ–∫–µ–Ω–æ–≤)
) -> list[Chunk]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ Docling-—ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ Chunk-–æ–±—ä–µ–∫—Ç—ã.
    –¢–∞–±–ª–∏—Ü—ã –∏ code-–±–ª–æ–∫–∏ ‚Äî –∞—Ç–æ–º–∞—Ä–Ω—ã–µ chunks.
    –¢–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è –¥–æ chunk_size —Å–∏–º–≤–æ–ª–æ–≤ —Å overlap.
    """
    chunks: list[Chunk] = []
    chunk_id = 0
    text_buffer = ""
    buffer_page = 1

    def flush_buffer(carry_over: str = "") -> None:
        nonlocal chunk_id, text_buffer, buffer_page
        if text_buffer.strip():
            chunks.append(Chunk(
                text=text_buffer.strip(),
                source_file=source_file,
                chunk_id=chunk_id,
                page_number=buffer_page,
                element_type="text",
            ))
            chunk_id += 1
        text_buffer = carry_over

    for element in elements:
        etype = element.get("type", "text")
        text = element.get("text", "")
        page = element.get("page", 1)

        if etype in ("table", "code"):
            # –°–Ω–∞—á–∞–ª–∞ —Å–±—Ä–æ—Å–∏—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±—É—Ñ–µ—Ä
            flush_buffer()
            if text.strip():
                chunks.append(Chunk(
                    text=text.strip(),
                    source_file=source_file,
                    chunk_id=chunk_id,
                    page_number=page,
                    element_type=etype,
                ))
                chunk_id += 1
        else:
            # –¢–µ–∫—Å—Ç–æ–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
            if not text_buffer:
                buffer_page = page
            text_buffer += text + " "

            while len(text_buffer) > chunk_size:
                # –ù–∞–π—Ç–∏ –≥—Ä–∞–Ω–∏—Ü—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —Ä–∞–π–æ–Ω–µ chunk_size
                cut = text_buffer.rfind(". ", 0, chunk_size)
                if cut == -1:
                    cut = chunk_size  # –Ω–µ—Ç —Ç–æ—á–∫–∏ ‚Äî —Ä–µ–∂–µ–º –∂—ë—Å—Ç–∫–æ
                else:
                    cut += 2  # –≤–∫–ª—é—á–∏—Ç—å ". "

                chunk_text = text_buffer[:cut].strip()
                carry = text_buffer[max(0, cut - overlap):cut]  # overlap —Å –∫–æ–Ω—Ü–∞
                chunks.append(Chunk(
                    text=chunk_text,
                    source_file=source_file,
                    chunk_id=chunk_id,
                    page_number=buffer_page,
                    element_type="text",
                ))
                chunk_id += 1
                text_buffer = carry + text_buffer[cut:]
                buffer_page = page

    flush_buffer()
    return chunks
```

**Step 4: –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã ‚Äî —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –ø—Ä–æ—Ö–æ–¥—è—Ç**

```bash
pytest tests/core/test_chunker.py -v
```

–û–∂–∏–¥–∞–µ–º: –≤—Å–µ —Ç–µ—Å—Ç—ã GREEN.

**Step 5: Commit**

```bash
git add core/chunker.py tests/core/test_chunker.py
git commit -m "feat: add chunker with atomic table/code support and text overlap"
```

---

## Task 2: core/embedder.py

**Files:**
- Create: `tests/core/test_embedder.py`
- Create: `core/embedder.py`

**Step 1: –ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ—Å—Ç—ã**

```python
# tests/core/test_embedder.py
import numpy as np
import pytest
from core.embedder import Embedder


def test_embedder_returns_numpy_array():
    embedder = Embedder()
    result = embedder.embed(["Hello world"])
    assert isinstance(result, np.ndarray)


def test_embedder_output_shape():
    embedder = Embedder()
    texts = ["Hello world", "Semantic search", "SQL query"]
    result = embedder.embed(texts)
    assert result.shape == (3, 384)  # all-MiniLM-L6-v2 ‚Üí 384 dimensions


def test_embedder_single_text():
    embedder = Embedder()
    result = embedder.embed(["Just one sentence"])
    assert result.shape == (1, 384)


def test_embedder_normalized_vectors():
    """–í–µ–∫—Ç–æ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (–¥–ª—è cosine similarity —á–µ—Ä–µ–∑ dot product)."""
    embedder = Embedder()
    result = embedder.embed(["Normalized vector test"])
    norms = np.linalg.norm(result, axis=1)
    np.testing.assert_allclose(norms, 1.0, atol=1e-5)


def test_similar_texts_have_high_similarity():
    embedder = Embedder()
    vecs = embedder.embed(["database schema", "schema of database", "python syntax"])
    sim_same = float(np.dot(vecs[0], vecs[1]))
    sim_diff = float(np.dot(vecs[0], vecs[2]))
    assert sim_same > sim_diff, "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"


def test_embedder_empty_list_returns_empty_array():
    embedder = Embedder()
    result = embedder.embed([])
    assert result.shape[0] == 0
```

**Step 2: –ó–∞–ø—É—Å—Ç–∏—Ç—å ‚Äî —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –ø–∞–¥–∞—é—Ç**

```bash
pytest tests/core/test_embedder.py -v
```

–û–∂–∏–¥–∞–µ–º: `ImportError`

**Step 3: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `core/embedder.py`**

```python
import numpy as np
from sentence_transformers import SentenceTransformer


class Embedder:
    """
    –û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç SentenceTransformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
    –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
    """

    def __init__(self, model_name: str = "all-MiniLM-L6-v2") -> None:
        self._model = SentenceTransformer(model_name)

    def embed(self, texts: list[str]) -> np.ndarray:
        """
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        Returns:
            np.ndarray shape (N, 384), –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (L2)
        """
        if not texts:
            return np.empty((0, 384), dtype=np.float32)

        embeddings = self._model.encode(
            texts,
            normalize_embeddings=True,  # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí dot product = cosine similarity
            show_progress_bar=False,
            convert_to_numpy=True,
        )
        return embeddings.astype(np.float32)
```

**Step 4: –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã**

```bash
pytest tests/core/test_embedder.py -v
```

–û–∂–∏–¥–∞–µ–º: –≤—Å–µ GREEN. (–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∏—Ç –º–æ–¥–µ–ª—å ~90MB ‚Äî –ø–æ–¥–æ–∂–¥–∞—Ç—å.)

**Step 5: Commit**

```bash
git add core/embedder.py tests/core/test_embedder.py
git commit -m "feat: add embedder wrapping all-MiniLM-L6-v2 with L2 normalization"
```

---

## Task 3: storage/file_storage.py + core/storage.py

**Files:**
- Create: `tests/storage/test_file_storage.py`
- Create: `core/storage.py`
- Create: `storage/file_storage.py`

**Step 1: –ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ—Å—Ç—ã**

```python
# tests/storage/test_file_storage.py
import json
import tempfile
from pathlib import Path

import numpy as np
import pytest

from core.chunker import Chunk
from storage.file_storage import FileStorage


def make_chunks(n=3, source="doc.pdf"):
    return [
        Chunk(
            text=f"chunk text {i}",
            source_file=source,
            chunk_id=i,
            page_number=1,
            element_type="text",
        )
        for i in range(n)
    ]


def make_embeddings(n=3, dim=384):
    vecs = np.random.rand(n, dim).astype(np.float32)
    # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    return vecs / norms


@pytest.fixture
def storage(tmp_path):
    return FileStorage(data_dir=tmp_path)


def test_storage_saves_and_loads_embeddings(storage):
    chunks = make_chunks(3)
    embeddings = make_embeddings(3)
    storage.save(chunks, embeddings)

    loaded_emb, loaded_meta = storage.load()
    assert loaded_emb.shape == (3, 384)
    np.testing.assert_allclose(loaded_emb, embeddings, atol=1e-6)


def test_storage_saves_metadata(storage):
    chunks = make_chunks(2)
    embeddings = make_embeddings(2)
    storage.save(chunks, embeddings)

    _, loaded_meta = storage.load()
    assert len(loaded_meta) == 2
    assert loaded_meta[0]["text"] == "chunk text 0"
    assert loaded_meta[0]["source_file"] == "doc.pdf"
    assert loaded_meta[0]["chunk_id"] == 0
    assert loaded_meta[0]["page_number"] == 1
    assert loaded_meta[0]["element_type"] == "text"


def test_storage_load_raises_when_empty(storage):
    with pytest.raises(FileNotFoundError):
        storage.load()


def test_storage_creates_data_dir_if_missing(tmp_path):
    new_dir = tmp_path / "new" / "nested"
    storage = FileStorage(data_dir=new_dir)
    chunks = make_chunks(1)
    embeddings = make_embeddings(1)
    storage.save(chunks, embeddings)
    assert (new_dir / "embeddings.npy").exists()


def test_storage_appends_new_chunks(storage):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º."""
    chunks1 = make_chunks(2, source="doc1.pdf")
    emb1 = make_embeddings(2)
    storage.save(chunks1, emb1)

    chunks2 = make_chunks(3, source="doc2.pdf")
    emb2 = make_embeddings(3)
    storage.append(chunks2, emb2)

    loaded_emb, loaded_meta = storage.load()
    assert loaded_emb.shape[0] == 5
    assert loaded_meta[4]["source_file"] == "doc2.pdf"


def test_storage_delete_by_source(storage):
    """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
    chunks = make_chunks(2, source="old.pdf") + make_chunks(2, source="keep.pdf")
    # –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö 4 —á–∞–Ω–∫–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏
    for i, c in enumerate(chunks):
        c.chunk_id = i
    emb = make_embeddings(4)
    storage.save(chunks, emb)

    storage.delete_by_source("old.pdf")

    loaded_emb, loaded_meta = storage.load()
    assert loaded_emb.shape[0] == 2
    assert all(m["source_file"] == "keep.pdf" for m in loaded_meta)


def test_storage_search_returns_top_k(storage):
    """cosine similarity –ø–æ–∏—Å–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç top-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
    chunks = make_chunks(10)
    emb = make_embeddings(10)
    storage.save(chunks, emb)

    query = make_embeddings(1)[0]
    results = storage.search(query_embedding=query, top_k=3)

    assert len(results) == 3
    # –ö–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: (chunk_metadata, score)
    for meta, score in results:
        assert "text" in meta
        assert 0.0 <= score <= 1.0


def test_storage_search_sorted_by_score(storage):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é score."""
    chunks = make_chunks(5)
    emb = make_embeddings(5)
    storage.save(chunks, emb)

    query = emb[2]  # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Ç—Ä–µ—Ç—å–∏–º –≤–µ–∫—Ç–æ—Ä–æ–º
    results = storage.search(query_embedding=query, top_k=3)

    scores = [score for _, score in results]
    assert scores == sorted(scores, reverse=True)
    assert results[0][1] > 0.99  # –ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
```

**Step 2: –ó–∞–ø—É—Å—Ç–∏—Ç—å ‚Äî —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –ø–∞–¥–∞—é—Ç**

```bash
pytest tests/storage/test_file_storage.py -v
```

**Step 3: –°–æ–∑–¥–∞—Ç—å `core/storage.py` (–∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è)**

```python
# core/storage.py
from typing import Protocol
import numpy as np
from core.chunker import Chunk


class StorageBackend(Protocol):
    """
    –ü—Ä–æ—Ç–æ–∫–æ–ª —Ö—Ä–∞–Ω–∏–ª–∏—â–∞. –†–µ–∞–ª–∏–∑—É–µ—Ç—Å—è:
    - storage.file_storage.FileStorage (MVP)
    - storage.db_storage.DBStorage (–≠—Ç–∞–ø 2, pgvector)
    """

    def save(self, chunks: list[Chunk], embeddings: np.ndarray) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å chunks –∏ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—å)."""
        ...

    def append(self, chunks: list[Chunk], embeddings: np.ndarray) -> None:
        """–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ chunks –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º."""
        ...

    def load(self) -> tuple[np.ndarray, list[dict]]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ. Raises FileNotFoundError –µ—Å–ª–∏ –ø—É—Å—Ç–æ."""
        ...

    def delete_by_source(self, source_file: str) -> None:
        """–£–¥–∞–ª–∏—Ç—å –≤—Å–µ chunks –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
        ...

    def search(
        self, query_embedding: np.ndarray, top_k: int = 5
    ) -> list[tuple[dict, float]]:
        """–ù–∞–π—Ç–∏ top_k –±–ª–∏–∂–∞–π—à–∏—Ö chunks –ø–æ cosine similarity."""
        ...
```

**Step 4: –°–æ–∑–¥–∞—Ç—å `storage/file_storage.py`**

```python
# storage/file_storage.py
import json
from pathlib import Path

import numpy as np

from core.chunker import Chunk


def _chunk_to_meta(chunk: Chunk) -> dict:
    return {
        "text": chunk.text,
        "source_file": chunk.source_file,
        "chunk_id": chunk.chunk_id,
        "page_number": chunk.page_number,
        "element_type": chunk.element_type,
    }


class FileStorage:
    """
    NumPy-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ: embeddings.npy (N √ó 384) + metadata.json.
    –†–µ–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª StorageBackend.
    """

    EMB_FILE = "embeddings.npy"
    META_FILE = "metadata.json"

    def __init__(self, data_dir: str | Path = "data") -> None:
        self._dir = Path(data_dir)

    def _emb_path(self) -> Path:
        return self._dir / self.EMB_FILE

    def _meta_path(self) -> Path:
        return self._dir / self.META_FILE

    def save(self, chunks: list[Chunk], embeddings: np.ndarray) -> None:
        self._dir.mkdir(parents=True, exist_ok=True)
        np.save(self._emb_path(), embeddings)
        with open(self._meta_path(), "w", encoding="utf-8") as f:
            json.dump([_chunk_to_meta(c) for c in chunks], f, ensure_ascii=False, indent=2)

    def append(self, chunks: list[Chunk], embeddings: np.ndarray) -> None:
        try:
            existing_emb, existing_meta = self.load()
            new_emb = np.vstack([existing_emb, embeddings])
            new_meta = existing_meta + [_chunk_to_meta(c) for c in chunks]
        except FileNotFoundError:
            new_emb = embeddings
            new_meta = [_chunk_to_meta(c) for c in chunks]

        self._dir.mkdir(parents=True, exist_ok=True)
        np.save(self._emb_path(), new_emb)
        with open(self._meta_path(), "w", encoding="utf-8") as f:
            json.dump(new_meta, f, ensure_ascii=False, indent=2)

    def load(self) -> tuple[np.ndarray, list[dict]]:
        if not self._emb_path().exists():
            raise FileNotFoundError(f"–•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {self._emb_path()}")
        embeddings = np.load(self._emb_path())
        with open(self._meta_path(), encoding="utf-8") as f:
            metadata = json.load(f)
        return embeddings, metadata

    def delete_by_source(self, source_file: str) -> None:
        embeddings, metadata = self.load()
        keep = [i for i, m in enumerate(metadata) if m["source_file"] != source_file]
        if not keep:
            # –ü—É—Å—Ç–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ ‚Äî —É–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            self._emb_path().unlink(missing_ok=True)
            self._meta_path().unlink(missing_ok=True)
            return
        new_emb = embeddings[keep]
        new_meta = [metadata[i] for i in keep]
        np.save(self._emb_path(), new_emb)
        with open(self._meta_path(), "w", encoding="utf-8") as f:
            json.dump(new_meta, f, ensure_ascii=False, indent=2)

    def search(
        self, query_embedding: np.ndarray, top_k: int = 5
    ) -> list[tuple[dict, float]]:
        """
        –õ–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ cosine similarity.
        query_embedding –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω (L2).
        –ü–æ—Å–∫–æ–ª—å–∫—É –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏,
        cosine_sim = dot(query, stored_vector).
        """
        embeddings, metadata = self.load()
        scores: np.ndarray = embeddings @ query_embedding  # (N,)
        top_indices = np.argsort(scores)[::-1][:top_k]
        return [(metadata[i], float(scores[i])) for i in top_indices]
```

**Step 5: –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã**

```bash
pytest tests/storage/test_file_storage.py -v
```

–û–∂–∏–¥–∞–µ–º: –≤—Å–µ GREEN.

**Step 6: Commit**

```bash
git add core/storage.py storage/file_storage.py tests/storage/test_file_storage.py
git commit -m "feat: add NumPy file storage with cosine similarity search"
```

---

## Task 4: core/parser.py

**Files:**
- Create: `tests/core/test_parser.py`
- Create: `core/parser.py`

### –ß—Ç–æ –¥–µ–ª–∞–µ—Ç parser

–ü—Ä–∏–Ω–∏–º–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ elements `{"text": str, "type": str, "page": int}`.
–¢–∏–ø—ã: `"text"`, `"table"`, `"code"`.

Docling API (v2):
```python
from docling.document_converter import DocumentConverter
converter = DocumentConverter()
result = converter.convert(str(path))
doc = result.document  # DoclingDocument
```

–î–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º –∏—Å–ø–æ–ª—å–∑—É–µ–º `doc.export_to_dict()` –∏–ª–∏ `doc.iterate_items()`.

**Step 1: –ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ—Å—Ç—ã (—Å –º–æ–∫–æ–º Docling)**

```python
# tests/core/test_parser.py
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from core.parser import Parser


@pytest.fixture
def mock_docling_result():
    """–ú–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ DocumentConverter.convert()"""
    mock_result = MagicMock()
    mock_doc = MagicMock()

    # –°–∏–º—É–ª–∏—Ä—É–µ–º iterate_items() ‚Üí —Å–ø–∏—Å–æ–∫ (item, level) –ø–∞—Ä
    text_item = MagicMock()
    text_item.text = "This is a paragraph about databases."
    text_item.__class__.__name__ = "TextItem"

    table_item = MagicMock()
    table_item.export_to_markdown.return_value = "| col1 | col2 |\n|------|------|\n| A    | B    |"
    table_item.__class__.__name__ = "TableItem"

    code_item = MagicMock()
    code_item.text = "SELECT * FROM users;"
    code_item.__class__.__name__ = "CodeItem"

    mock_doc.iterate_items.return_value = [
        (text_item, 0),
        (table_item, 0),
        (code_item, 0),
    ]
    mock_result.document = mock_doc
    return mock_result


def test_parser_returns_list_of_elements(mock_docling_result, tmp_path):
    fake_file = tmp_path / "test.pdf"
    fake_file.write_bytes(b"fake pdf content")

    with patch("core.parser.DocumentConverter") as MockConverter:
        MockConverter.return_value.convert.return_value = mock_docling_result
        parser = Parser()
        elements = parser.parse(fake_file)

    assert isinstance(elements, list)
    assert len(elements) == 3


def test_parser_text_element(mock_docling_result, tmp_path):
    fake_file = tmp_path / "test.pdf"
    fake_file.write_bytes(b"fake")

    with patch("core.parser.DocumentConverter") as MockConverter:
        MockConverter.return_value.convert.return_value = mock_docling_result
        parser = Parser()
        elements = parser.parse(fake_file)

    assert elements[0]["type"] == "text"
    assert "databases" in elements[0]["text"]


def test_parser_table_element(mock_docling_result, tmp_path):
    fake_file = tmp_path / "test.pdf"
    fake_file.write_bytes(b"fake")

    with patch("core.parser.DocumentConverter") as MockConverter:
        MockConverter.return_value.convert.return_value = mock_docling_result
        parser = Parser()
        elements = parser.parse(fake_file)

    assert elements[1]["type"] == "table"
    assert "col1" in elements[1]["text"]


def test_parser_code_element(mock_docling_result, tmp_path):
    fake_file = tmp_path / "test.pdf"
    fake_file.write_bytes(b"fake")

    with patch("core.parser.DocumentConverter") as MockConverter:
        MockConverter.return_value.convert.return_value = mock_docling_result
        parser = Parser()
        elements = parser.parse(fake_file)

    assert elements[2]["type"] == "code"
    assert "SELECT" in elements[2]["text"]


def test_parser_raises_for_missing_file():
    parser = Parser()
    with pytest.raises(FileNotFoundError):
        parser.parse(Path("/nonexistent/file.pdf"))


def test_parser_raises_for_unsupported_format(tmp_path):
    bad_file = tmp_path / "test.xyz"
    bad_file.write_text("content")
    parser = Parser()
    with pytest.raises(ValueError, match="–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"):
        parser.parse(bad_file)
```

**Step 2: –ó–∞–ø—É—Å—Ç–∏—Ç—å ‚Äî —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –ø–∞–¥–∞—é—Ç**

```bash
pytest tests/core/test_parser.py -v
```

**Step 3: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `core/parser.py`**

```python
# core/parser.py
from pathlib import Path
from typing import Any

SUPPORTED_EXTENSIONS = {".pdf", ".docx", ".md", ".txt"}


class Parser:
    """
    –û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç Docling DocumentConverter.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {"text": str, "type": str, "page": int}
    """

    def __init__(self) -> None:
        # –õ–µ–Ω–∏–≤—ã–π –∏–º–ø–æ—Ä—Ç ‚Äî Docling —Ç—è–∂—ë–ª—ã–π, –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        from docling.document_converter import DocumentConverter
        self._converter = DocumentConverter()

    def parse(self, file_path: str | Path) -> list[dict[str, Any]]:
        path = Path(file_path)

        if not path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

        if path.suffix.lower() not in SUPPORTED_EXTENSIONS:
            raise ValueError(
                f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {path.suffix}. "
                f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: {', '.join(SUPPORTED_EXTENSIONS)}"
            )

        result = self._converter.convert(str(path))
        doc = result.document

        elements: list[dict[str, Any]] = []

        for item, _level in doc.iterate_items():
            class_name = item.__class__.__name__

            if class_name == "TableItem":
                try:
                    text = item.export_to_markdown()
                except Exception:
                    text = str(item)
                elements.append({"text": text, "type": "table", "page": 1})

            elif class_name == "CodeItem":
                text = getattr(item, "text", str(item))
                elements.append({"text": text, "type": "code", "page": 1})

            elif hasattr(item, "text") and item.text:
                elements.append({"text": item.text, "type": "text", "page": 1})

        return elements
```

> **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** Docling v2 API –º–æ–∂–µ—Ç –Ω–µ–º–Ω–æ–≥–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è. –ï—Å–ª–∏ —Ç–µ—Å—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏ –ø–∞–¥–∞—é—Ç, –ø—Ä–æ–≤–µ—Ä—å `doc.iterate_items()` –≤ Docling docs: https://ds4sd.github.io/docling/

**Step 4: –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã**

```bash
pytest tests/core/test_parser.py -v
```

–û–∂–∏–¥–∞–µ–º: –≤—Å–µ GREEN.

**Step 5: Commit**

```bash
git add core/parser.py tests/core/test_parser.py
git commit -m "feat: add Docling parser with text/table/code element extraction"
```

---

## Task 5: CLI Commands

**Files:**
- Create: `tests/test_cli.py`
- Create: `cli/__init__.py` (main entry point)
- Create: `cli/commands.py`
- Create: `cli/config_loader.py`

**Step 1: –ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ—Å—Ç—ã CLI**

```python
# tests/test_cli.py
import json
from pathlib import Path
from unittest.mock import MagicMock, patch

import numpy as np
import pytest
from click.testing import CliRunner

from cli import main


@pytest.fixture
def runner():
    return CliRunner()


@pytest.fixture
def initialized_storage(tmp_path):
    """–°–æ–∑–¥–∞—ë–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Ç–µ—Å—Ç–æ–≤."""
    (tmp_path / "data").mkdir()
    (tmp_path / "logs").mkdir()
    return tmp_path


def test_init_command_creates_data_dir(runner, tmp_path):
    result = runner.invoke(main, ["init", "--data-dir", str(tmp_path / "mystore")])
    assert result.exit_code == 0
    assert (tmp_path / "mystore").exists()
    assert "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ" in result.output


def test_list_command_empty_storage(runner, tmp_path):
    runner.invoke(main, ["init", "--data-dir", str(tmp_path)])
    result = runner.invoke(main, ["list", "--data-dir", str(tmp_path)])
    assert result.exit_code == 0
    assert "–ø—É—Å—Ç–æ–µ" in result.output.lower() or "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç" in result.output.lower()


def test_add_command_indexes_file(runner, tmp_path):
    """add –¥–æ–ª–∂–µ–Ω —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –Ω–∞—Ä–µ–∑–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏."""
    test_doc = tmp_path / "test.md"
    test_doc.write_text("# Test\n\nThis is a test document about databases and SQL.\n")

    with (
        patch("cli.commands.Parser") as MockParser,
        patch("cli.commands.Embedder") as MockEmbedder,
        patch("cli.commands.FileStorage") as MockStorage,
    ):
        mock_elements = [{"text": "Test content about databases.", "type": "text", "page": 1}]
        MockParser.return_value.parse.return_value = mock_elements
        MockEmbedder.return_value.embed.return_value = np.ones((1, 384), dtype=np.float32)

        mock_storage_instance = MagicMock()
        mock_storage_instance.load.side_effect = FileNotFoundError
        MockStorage.return_value = mock_storage_instance

        result = runner.invoke(main, ["add", str(test_doc), "--data-dir", str(tmp_path)])

    assert result.exit_code == 0
    assert "–î–æ–±–∞–≤–ª–µ–Ω" in result.output or "chunk" in result.output.lower()


def test_search_command_returns_results(runner, tmp_path):
    """search –¥–æ–ª–∂–µ–Ω –≤—ã–≤–µ—Å—Ç–∏ —Ç–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å score."""
    mock_results = [
        ({"text": "SQL query example SELECT *", "source_file": "doc.pdf",
          "chunk_id": 0, "page_number": 1, "element_type": "code"}, 0.92),
        ({"text": "Database schema description", "source_file": "arch.docx",
          "chunk_id": 1, "page_number": 2, "element_type": "text"}, 0.78),
    ]

    with (
        patch("cli.commands.Embedder") as MockEmbedder,
        patch("cli.commands.FileStorage") as MockStorage,
    ):
        MockEmbedder.return_value.embed.return_value = np.ones((1, 384), dtype=np.float32)
        MockStorage.return_value.search.return_value = mock_results

        result = runner.invoke(
            main, ["search", "SQL query example", "--data-dir", str(tmp_path)]
        )

    assert result.exit_code == 0
    assert "0.92" in result.output or "92" in result.output
    assert "doc.pdf" in result.output


def test_search_command_empty_storage(runner, tmp_path):
    with (
        patch("cli.commands.Embedder") as MockEmbedder,
        patch("cli.commands.FileStorage") as MockStorage,
    ):
        MockEmbedder.return_value.embed.return_value = np.ones((1, 384), dtype=np.float32)
        MockStorage.return_value.search.side_effect = FileNotFoundError

        result = runner.invoke(main, ["search", "query", "--data-dir", str(tmp_path)])

    assert result.exit_code == 0
    assert "–ø—É—Å—Ç–æ–µ" in result.output.lower() or "–Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" in result.output.lower()
```

**Step 2: –ó–∞–ø—É—Å—Ç–∏—Ç—å ‚Äî —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –ø–∞–¥–∞—é—Ç**

```bash
pytest tests/test_cli.py -v
```

**Step 3: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `cli/config_loader.py`**

```python
# cli/config_loader.py
from pathlib import Path
import yaml

_DEFAULTS = {
    "embedding_model": "all-MiniLM-L6-v2",
    "chunk_size": 3200,
    "chunk_overlap": 320,
    "top_k_results": 5,
    "data_dir": "data",
    "log_file": "logs/search.log",
}


def load_config(config_path: str | Path = "config.yaml") -> dict:
    cfg = dict(_DEFAULTS)
    path = Path(config_path)
    if path.exists():
        with open(path, encoding="utf-8") as f:
            user_cfg = yaml.safe_load(f) or {}
        cfg.update(user_cfg)
    return cfg
```

**Step 4: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `cli/commands.py`**

```python
# cli/commands.py
import logging
from pathlib import Path

import click

from cli.config_loader import load_config
from core.chunker import chunk_elements
from core.embedder import Embedder
from core.parser import Parser
from storage.file_storage import FileStorage


def get_storage(data_dir: str) -> FileStorage:
    return FileStorage(data_dir=Path(data_dir))


@click.group()
def main() -> None:
    """docling-rag ‚Äî —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."""
    pass


@main.command()
@click.option("--data-dir", default="data", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
def init(data_dir: str) -> None:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ö—Ä–∞–Ω–∏–ª–∏—â–µ."""
    path = Path(data_dir)
    path.mkdir(parents=True, exist_ok=True)
    (path.parent / "logs").mkdir(exist_ok=True)
    click.echo(f"‚úì –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {path.resolve()}")


@main.command()
@click.argument("file_path", type=click.Path(exists=True))
@click.option("--data-dir", default="data", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
@click.option("--config", default="config.yaml", help="–ü—É—Ç—å –∫ config.yaml")
def add(file_path: str, data_dir: str, config: str) -> None:
    """–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –∏–Ω–¥–µ–∫—Å."""
    cfg = load_config(config)
    path = Path(file_path)
    files = list(path.rglob("*.*")) if path.is_dir() else [path]
    supported = {".pdf", ".docx", ".md", ".txt"}
    files = [f for f in files if f.suffix.lower() in supported]

    if not files:
        click.echo("–ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
        return

    parser = Parser()
    embedder = Embedder(model_name=cfg["embedding_model"])
    storage = get_storage(data_dir)

    total_chunks = 0
    for file in files:
        click.echo(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {file.name} ...", nl=False)
        try:
            elements = parser.parse(file)
            chunks = chunk_elements(
                elements,
                source_file=str(file),
                chunk_size=cfg["chunk_size"],
                overlap=cfg["chunk_overlap"],
            )
            if not chunks:
                click.echo(" (–ø—É—Å—Ç–æ–π –¥–æ–∫—É–º–µ–Ω—Ç, –ø—Ä–æ–ø—É—Å–∫–∞—é)")
                continue
            texts = [c.text for c in chunks]
            embeddings = embedder.embed(texts)
            storage.append(chunks, embeddings)
            total_chunks += len(chunks)
            click.echo(f" ‚úì {len(chunks)} chunks")
        except (ValueError, FileNotFoundError) as e:
            click.echo(f" ‚úó –û—à–∏–±–∫–∞: {e}")

    click.echo(f"\n–î–æ–±–∞–≤–ª–µ–Ω–æ {total_chunks} chunks –∏–∑ {len(files)} —Ñ–∞–π–ª–æ–≤.")


@main.command()
@click.argument("query")
@click.option("--data-dir", default="data", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
@click.option("--top-k", default=5, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
@click.option("--config", default="config.yaml", help="–ü—É—Ç—å –∫ config.yaml")
def search(query: str, data_dir: str, top_k: int, config: str) -> None:
    """–í—ã–ø–æ–ª–Ω–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."""
    cfg = load_config(config)
    embedder = Embedder(model_name=cfg["embedding_model"])
    storage = get_storage(data_dir)

    try:
        query_emb = embedder.embed([query])[0]
        results = storage.search(query_embedding=query_emb, top_k=top_k)
    except FileNotFoundError:
        click.echo("–•—Ä–∞–Ω–∏–ª–∏—â–µ –ø—É—Å—Ç–æ–µ. –î–æ–±–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã: docling-rag add <path>")
        return

    if not results:
        click.echo("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return

    click.echo(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è: \"{query}\"\n" + "‚îÄ" * 60)
    for i, (meta, score) in enumerate(results, 1):
        source = Path(meta["source_file"]).name
        page = meta.get("page_number", "?")
        etype = meta.get("element_type", "text")
        text_preview = meta["text"][:300].replace("\n", " ")
        click.echo(
            f"\n[{i}] score={score:.3f} | {source} | —Å—Ç—Ä.{page} | {etype}\n"
            f"    {text_preview}..."
        )

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    _log_search(cfg["log_file"], query, results[0][1] if results else 0.0)


@main.command("list")
@click.option("--data-dir", default="data", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
def list_docs(data_dir: str) -> None:
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    storage = get_storage(data_dir)
    try:
        _, metadata = storage.load()
    except FileNotFoundError:
        click.echo("–•—Ä–∞–Ω–∏–ª–∏—â–µ –ø—É—Å—Ç–æ–µ. –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç.")
        return

    sources = {}
    for m in metadata:
        src = m["source_file"]
        sources[src] = sources.get(src, 0) + 1

    click.echo(f"\n–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(sources)}\n" + "‚îÄ" * 60)
    for src, count in sorted(sources.items()):
        click.echo(f"  {Path(src).name:40s} {count:4d} chunks  ({src})")


def _log_search(log_file: str, query: str, top_score: float) -> None:
    from datetime import datetime
    path = Path(log_file)
    path.parent.mkdir(exist_ok=True)
    with open(path, "a", encoding="utf-8") as f:
        f.write(f"{datetime.now().isoformat()} | score={top_score:.3f} | {query}\n")
```

**Step 5: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `cli/__init__.py`**

```python
# cli/__init__.py
from cli.commands import main

__all__ = ["main"]
```

**Step 6: –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã**

```bash
pytest tests/test_cli.py -v
```

–û–∂–∏–¥–∞–µ–º: –≤—Å–µ GREEN.

**Step 7: –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Ç–µ—Å—Ç—ã**

```bash
pytest tests/ -v
```

–û–∂–∏–¥–∞–µ–º: –≤—Å–µ GREEN.

**Step 8: Commit**

```bash
git add cli/ tests/test_cli.py
git commit -m "feat: add CLI commands init/add/search/list with config support"
```

---

## Task 6: Integration Smoke Test

**Files:**
- Create: `tests/test_integration.py`

**Step 1: –ù–∞–ø–∏—Å–∞—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç**

```python
# tests/test_integration.py
"""
Smoke-—Ç–µ—Å—Ç: end-to-end –ø–∞–π–ø–ª–∞–π–Ω –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º .md —Ñ–∞–π–ª–µ.
–¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ Docling –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
–ü–æ–º–µ—Ç–∏—Ç—å @pytest.mark.integration ‚Äî –Ω–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –≤ CI –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
"""
import pytest
from pathlib import Path
from click.testing import CliRunner
from cli import main


@pytest.mark.integration
def test_full_pipeline_on_real_md(tmp_path):
    """add ‚Üí search –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º Markdown —Ñ–∞–π–ª–µ."""
    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    doc = tmp_path / "test_doc.md"
    doc.write_text(
        "# Database Architecture\n\n"
        "The DWH uses a star schema with fact and dimension tables.\n\n"
        "## SQL Example\n\n"
        "```sql\nSELECT customer_id, SUM(amount)\nFROM fact_sales\nGROUP BY customer_id;\n```\n",
        encoding="utf-8",
    )

    data_dir = str(tmp_path / "store")
    runner = CliRunner()

    # Init
    result = runner.invoke(main, ["init", "--data-dir", data_dir])
    assert result.exit_code == 0

    # Add
    result = runner.invoke(main, ["add", str(doc), "--data-dir", data_dir])
    assert result.exit_code == 0
    assert "chunk" in result.output.lower()

    # Search
    result = runner.invoke(main, ["search", "star schema fact table", "--data-dir", data_dir])
    assert result.exit_code == 0
    assert "score=" in result.output
    assert "test_doc.md" in result.output
```

**Step 2: –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ —é–Ω–∏—Ç-—Ç–µ—Å—Ç—ã (–±–µ–∑ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö)**

```bash
pytest tests/ -v -m "not integration"
```

–û–∂–∏–¥–∞–µ–º: –≤—Å–µ GREEN.

**Step 3: –ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –≤—Ä—É—á–Ω—É—é**

```bash
pytest tests/test_integration.py -v -m integration -s
```

–û–∂–∏–¥–∞–µ–º: PASS (–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ —Å–∫–∞—á–∞–µ—Ç –º–æ–¥–µ–ª—å).

**Step 4: –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–º–º–∏—Ç**

```bash
git add tests/test_integration.py
git commit -m "test: add integration smoke test for full add‚Üísearch pipeline"
```

---

## –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞

```bash
# –í—Å–µ —Ç–µ—Å—Ç—ã (–∫—Ä–æ–º–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö)
pytest tests/ -v -m "not integration"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ CLI
docling-rag --help
docling-rag init --help
docling-rag add --help
docling-rag search --help
docling-rag list --help
```

---

## –ß—Ç–æ –¥–∞–ª—å—à–µ (P1, –ø–æ—Å–ª–µ MVP)

- `R-6: Skills –¥–ª—è AI` ‚Äî —Å–æ–∑–¥–∞—Ç—å `skills/docling-rag.md` —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –¥–ª—è Claude Code
- `R-7: docling-rag update <file>` ‚Äî –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (—É–∂–µ –µ—Å—Ç—å `delete_by_source` + `append`)
- `R-8: config.yaml` ‚Äî —É–∂–µ –µ—Å—Ç—å –±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å
